{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def stimami_la_RAL(linguaggio, is_manager, istruzione, anni_esp):\n",
    "    ral = 20000 # minimo\n",
    "    \n",
    "    # aggiungo il peso riferito al linguaggio\n",
    "    ral += ral * pesi_linguaggio[linguaggio]\n",
    "    \n",
    "    # aggiungo il peso riferito agli anni di esperienza\n",
    "    ral += ral * pesi_anni[anni_esp]\n",
    "    \n",
    "    # aggiungo il peso riferito al grado di istruzione\n",
    "    ral += ral * pesi_istruzione[istruzione]\n",
    "    \n",
    "    # Ã¨ un responsabile?\n",
    "    if is_manager:\n",
    "        ral += ral * peso_manager\n",
    "    \n",
    "    return ral\n",
    "\n",
    "def calcola_stima(riga): \n",
    "    ral = stimami_la_RAL( riga['Linguaggio'], riga['Responsabile'], riga['Istruzione'], riga['Esperienza'] )\n",
    "    return ral\n",
    "\n",
    "df = pd.read_csv('DataLab_RAL/DataLab_RAL.csv')\n",
    "\n",
    "df.head(10)\n",
    "\n",
    "# Preliminary test\n",
    "# ----------------------\n",
    "raggruppamenti = ['Linguaggio', 'Responsabile', 'Istruzione', 'Esperienza']\n",
    "\n",
    "agg = df.groupby(raggruppamenti).agg('mean')\n",
    "df_agg = agg.reset_index()\n",
    "df_agg[df_agg['Linguaggio'] == 'Python']\n",
    "\n",
    "#print(df.loc[(df['Linguaggio']=='Python') & (df['Esperienza']==2) & (df['Istruzione']=='Dottorato')])\n",
    "#ral = stimami_la_RAL('Python', 0, 'Dottorato', 2)\n",
    "\n",
    "pesi_linguaggio = { 'Python': 0.05, 'Javascript': 0.01, 'Java': 0.03, 'R': 0.07, 'C#': 0.02, \n",
    "                   'Objective-C/Swift': 0.03, 'SQL': 0.02, 'COBOL': 0.04 }\n",
    "\n",
    "pesi_istruzione = { 'Diploma': 0, 'Laurea Triennale': 0.02, 'Laurea Magistrale': 0.03, 'Dottorato': 0.05 }\n",
    "\n",
    "def pesi_anni(anni_di_esperienza):\n",
    "    if anni_di_esperienza > 8:\n",
    "        return 1 \n",
    "    elif 6 < anni_di_esperienza <= 8:\n",
    "        return 0.7\n",
    "    elif 4 < anni_di_esperienza <= 6:\n",
    "        return 0.5\n",
    "    elif 1 < anni_di_esperienza <= 4:\n",
    "        return 0.25\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "pesi_anni = {anni: pesi_anni(anni) for anni in range(1, 41)}\n",
    "\n",
    "peso_manager = 0.5\n",
    "\n",
    "df['Stima'] = df.apply(calcola_stima, axis=1)\n",
    "\n",
    "# ----------------------\n",
    "\n",
    "# parameters \n",
    "alpha = 0.04\n",
    "numIter = 1000\n",
    "\n",
    "minRAL = 20000\n",
    "\n",
    "print(df.loc[df['Linguaggio']=='Python']) \n",
    "\n",
    "# Convert columns of strings into numbers\n",
    "\n",
    "linguaggi_enc = preprocessing.LabelEncoder()\n",
    "istruzione_enc = preprocessing.LabelEncoder()\n",
    "linguaggi = linguaggi_enc.fit(df['Linguaggio'])\n",
    "istruzione = istruzione_enc.fit(df['Istruzione'])\n",
    "df['Linguaggio'] = linguaggi.transform(df['Linguaggio'])\n",
    "df['Istruzione'] = istruzione.transform(df['Istruzione'])\n",
    "\n",
    "X = df[ ['Linguaggio', 'Esperienza', 'Istruzione', 'Responsabile'] ].values\n",
    "\n",
    "# Feature scaling to get a better convergence of the gradientDescent method\n",
    "\n",
    "#avg = []\n",
    "#std = []\n",
    "#    avg.append(np.mean(df[elemento]))\n",
    "#    std.append(np.std(df[elemento]))\n",
    "#for elemento in raggruppamenti:\n",
    "#    df[elemento] = (df[elemento] - np.mean(df[elemento])) / np.std(df[elemento])\n",
    "\n",
    "#Xscal = df[ ['Linguaggio', 'Esperienza', 'Istruzione', 'Responsabile'] ].values\n",
    "#avg = np.mean(df[ ['Linguaggio', 'Esperienza', 'Istruzione', 'Responsabile'] ]).values\n",
    "#std = np.std(df[ ['Linguaggio', 'Esperienza', 'Istruzione', 'Responsabile'] ]).values\n",
    "\n",
    "y = df['RAL'].values\n",
    "\n",
    "m, n = np.shape(X)\n",
    "\n",
    "# hypothesis function h(x) = theta0 + theta1*x1 + theta2*x2 + ...     where x0 = 1 and theta0 = 20000\n",
    "X = np.concatenate((np.ones((m,1), dtype=np.int), X.reshape(m,n)), axis=1)\n",
    "#Xscal = np.concatenate((np.ones((m,1), dtype=np.int), Xscal.reshape(m,n)), axis=1)\n",
    "theta0 = np.ones(n+1)\n",
    "theta0[0] = minRAL\n",
    "# GradientDescent method\n",
    "\n",
    "# ---------------------------------\n",
    "\n",
    "def GradientDescent(X, y, theta, alpha, m, numIter): \n",
    "    Jcycle = []\n",
    "    for i in range(0, numIter): \n",
    "        # hypothesis function h(x) = theta*X' --> error = h(x) - y ---> Jcost = sum(err^2) / 2m\n",
    "        err = np.dot(X,theta) - y\n",
    "        Jcost = np.sum(err ** 2)/(2 * m)\n",
    "        Jcycle.append((i, Jcost))\n",
    "        grad = np.dot(X.transpose(), err) / m\n",
    "        theta = theta - alpha * grad\n",
    "    return Jcycle,theta\n",
    "\n",
    "Jcycle, thetaGD = GradientDescent(X, y, theta0, alpha, m, numIter)\n",
    "\n",
    "# Normal equation method ---> theta = (X.T*X)^{-1}*X.T*y [Matrix form where X.T is the transpose form of the matrix X]\n",
    "\n",
    "# Moore-Penrose pseudo-inverse of a matrix ---> pinv = (X.T*X)^{-1}*X.T\n",
    "\n",
    "pinv = np.linalg.pinv(X) \n",
    "thetaNE = np.dot(pinv,y)\n",
    "\n",
    "# J vs #iterations plot\n",
    "\n",
    "# ---------------------------------\n",
    "\n",
    "#itera = [row[0] for row in Jcycle];\n",
    "#cost = [row[1] for row in Jcycle];\n",
    "#plt.plot(itera, cost, label = 'Jcost')\n",
    "#plt.xlabel('number of iterations')\n",
    "#plt.ylabel('Jcost')\n",
    "#plt.title('The cost function vs number of iterations')\n",
    "#plt.legend(loc = 'upper right');\n",
    "\n",
    "# ---------------------------------\n",
    "\n",
    "# Similarity between the theta matrix between the two methods (i.e. thetaGD == thetaNE)\n",
    "\n",
    "#print(\"The parameters evaluated with the use of the Gradient Descent method are : \",thetaGD)\n",
    "#print(\"The parameters evaluated with the use of the Normal Equation method (exact solution) are : \",thetaNE)\n",
    "\n",
    "# Evaluate the RAL of a new employee using the calculated theta parameters\n",
    "\n",
    "def stima_regressione(X,theta):\n",
    "    return np.dot(X,theta)\n",
    "\n",
    "linguaggio = 5 #-avg[0]) / std[0]# Python\n",
    "esperienza = 1 #- avg[1]) / std[1]\n",
    "istruzione = 3 #- avg[2]) / std[2] # Laurea Triennale\n",
    "responsabile = 0 #- avg[3]) / std[3]\n",
    "\n",
    "# hypothesis function h(x) = theta0 + theta1*x1 + theta2*x2 + ...     where x0 = 1\n",
    "\n",
    "nuovo_impiegato = [1, linguaggio, esperienza, istruzione, responsabile]\n",
    "\n",
    "print(\"\\n RAL for the user (Pyt,Esp=1,Trien,Res=0) using Gradient Descent : \", stima_regressione(nuovo_impiegato, thetaGD))\n",
    "print(\"\\n RAL for the user (Pyt,Esp=1,Trien,Res=0) using Normal Equation : \", stima_regressione(nuovo_impiegato, thetaNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
